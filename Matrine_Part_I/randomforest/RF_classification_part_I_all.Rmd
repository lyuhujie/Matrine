---
title: "Random Forest Classification(随机森林分类)"
author: "Yong-Xin Liu(刘永鑫)"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
    theme: cerulean
    highlight: haddock
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: no
      smooth_scroll: yes
    code_fold: show
---
 
```{r setup, include=T}
knitr::opts_chunk$set(echo = TRUE)
# 检测和安装依赖包
package_list <- c("randomForest","ggplot2","pheatmap")
# 判断R包加载是否成功来决定是否安装后再加载
for(p in package_list){
  if(!suppressWarnings(suppressMessages(require(p, character.only = TRUE, quietly = TRUE, warn.conflicts = FALSE)))){
    install.packages(p)
    suppressWarnings(suppressMessages(library(p, character.only = TRUE, quietly = TRUE, warn.conflicts = FALSE)))
  }
}

```

随机森林分类通常有两个实验组，本示例原文于2019年由作者发表于Nature Biotechnology封面文章，以水稻籼稻(IND)和粳稻(TEJ)((在医学中如：健康人Healthy和癌症Cancer)。这些样本分别收集于L地和H地两地。我们计划以L地样本为建立模型、5倍交叉验证筛选重要的特征，再用H样本进行异地验证证明模型的普适性。引文和数据出处见文末引文。


format2stamp步骤

```{r}
# 1. 读取OTU表
otutab = read.table("otutab.txt", header=T, row.names= 1, sep="\t", comment.char = "", stringsAsFactors = F)
# 2. 读取物种注释
tax = read.table("taxonomy.txt", header=T, row.names= 1, sep="\t",comment.char = "", stringsAsFactors = F) 
# 数据按实验设计手动筛选(可选)
metadata_all = read.table("metadata.txt", header=T, row.names=1, sep="\t") 


# 实验设计分组。根据group分组
State_A = "BHD"
State_B = "CMD"
# CLMAT, DHMAT
#变化下Sday_C变为pre
filetial1 = "rf.txt"
filetial2 = "rf.pdf"
sub_metadatanames = "sub_metadata"
rfcv_names = "rfcv"
importance_names = "importance"
top_f = "top_feautres"
prediction_name = "prediction_binary"
test_names = "test"
class_names = "A"
result_name = "result"

result_file_names <- paste(result_name,State_A,State_B,sep ="_")
sub_metadata_filename <- paste(sub_metadatanames,State_A,State_B,filetial1,sep ="_")
rfcv_filename_txt <- paste(class_names,rfcv_names,State_A,State_B,filetial1,sep ="_")
rfcv_filename_pdf <- paste(class_names,rfcv_names,State_A,State_B,filetial2,sep ="_")
importance_filename_txt <- paste(class_names,importance_names,State_A,State_B,filetial1,sep ="_")
top_feautre_filename <- paste(class_names,top_f,State_A,State_B,filetial2,sep ="_")
prediction_filename <- paste(class_names,prediction_name,State_A,State_B,filetial1,sep ="_")
testfilename1 <- paste(class_names,test_names,State_A,filetial2,sep ="_")
testfilename2 <- paste(class_names,test_names,State_B,filetial2,sep ="_")

print(result_file_names)
print(sub_metadata_filename)
print(rfcv_filename_txt)
print(rfcv_filename_pdf)
print(importance_filename_txt)
print(top_feautre_filename)
print(prediction_filename)
print(testfilename1)
print(testfilename2)



# 筛选自己想要的分组以及内容

sub_metadata2 = subset(metadata_all, Group_big %in% c(State_A,State_B))
sub_metadata2



# 实验设计与输入文件交叉筛选。
idx = rownames(sub_metadata2) %in% colnames(otutab)
sub_metadata2 = sub_metadata2[idx,]
sub_metadata2
sub_otutab_tax = otutab[,rownames(sub_metadata2)]
sub_otutab_tax
write.table(sub_metadata2,file = sub_metadata_filename,quote = F,sep = '\t', row.names = T, col.names = T)


# OTU丰度筛选阈值，默认0.1%，0为来筛选
thre = 0.02
# 输出文件名前缀
prefix = "tax_BHD_CMD"

# 生成各分类级汇总特征表
suppressWarnings(suppressMessages(library(amplicon)))
format2stamp(sub_otutab_tax, tax, thre, prefix)
# 在当前目录生成tax_1-8共7个级别+OTU过滤文件

```

## 样本随机分组(可选)

注：数据可以自由分组，如选择50%-80%建模，其余部分验证。为提高模型准确性，可以整合更多来源的数据，如北京、上海中一半作模型，另一半做验证，有可能提高验证时预测的准确率。随机取样代码如下：

```{r}
# 假设从一百个样本中随机取70个，且无放回
# idx = sample(1:100, size = 70, replace = F)
# # 选择的样本标记为TRUE，未选择的为FALSE
# idx = 1:100 %in% idx
# 再用这个索引idx筛选对应的数据表，一部分作为训练集(train)，另一部分作为测试集(test)
# train=metadata[idx,]
# test=metadata[!idx,]

#### 有自己设计好的训练集就不需要运行这一步。

# 自己设计
# idx1 = sample(1:214, size = 140, replace = F)
# # 选择的样本标记为TRUE，未选择的为FALSE
# idx = 1:214 %in% idx1
# idx

```

## 分类级选择(可选)

先使用format2stamp.Rmd基于OTU表(otutab.txt)、物种注释(taxonomy.txt)和元数据(metadata.txt)筛选样本、高丰度特征，并分类汇总各分类级(tax_1-8)。然后对各分类级进行准确性评估

```{r}
# 读取实验设计、和物种分类文件
# getwd()  #显示当前工作目录
# setwd("C:/16S/result") 
# metadata_f = read.table("metadata_DSmed_HSmed.txt",header = T, row.names = 1)
# 实验用的数据重命名
metadata_f = sub_metadata2
metadata_f

# 设置训练集
train = subset(metadata_f, Set %in% c("BHDA", "CMDA"))
train

train2 = train
train2
train2$Group_big = as.factor(train2$Group_big)
summary(train2)

# 筛选一个合适的分类等级
library(randomForest)
#set.seed(88)
# 自己运行
for(i in c("2Phylum","3Class","4Order","5Family","6Genus","8OTU0.02")){
  # i="5Family"
  set.seed(0)
  table_f = read.table(paste0("tax_BHD_CMD_",i,".txt"),header = T, row.names = 1)
  table_f = table_f[,rownames(train2)]
  train2$Group_big = as.factor(train2$Group_big)
  rf = randomForest(t(table_f), train2$Group_big, importance=T, proximity=T, ntree = 1000)
  print(i)
  print(rf)
}
# OOB estimate of  error rate: 越小越好。

# OTU最低最适合

# 本次观察到科水平最准确，以后使用科水平分析，可以将筛选的结果做成拆线图作为附图
```

## 最佳水平数据读取和统计

读取实验设计、Feature表，并进行数据筛选和交叉筛选

```{r}

# 根据之前筛选情况读取物种分类水平数据。读取科水平特征表
table =read.table("tax_BHD_CMD_6Genus.txt",header = T, row.names = 1)
# train2重命名，方便后面区分。
metadata_train1 = train2
summary(metadata_train1)

# 筛选OTU
idx = rownames(metadata_train1) %in% colnames(table)
metadata_train1 = metadata_train1[idx,]
otu_sub1 = table[, rownames(metadata_train1)] 
otu_sub1
dim(otu_sub1)
```


## 选择最佳随机数(可选)

```{r}
library(randomForest)
set.seed(77)
for (i in 0:9){
  set.seed(i)
  rf2 = randomForest(t(otu_sub1), metadata_train1$Group_big, importance=TRUE, proximity=TRUE, ntree = 1000)
  print(i)
  print(rf2)
}

```

## 随机森林分类

在确定的分类层级和最佳随机数下建模

```{r}
library(randomForest)
set.seed(1)
rf3 = randomForest(t(otu_sub1), metadata_train1$Group_big, importance=TRUE, proximity=TRUE, ntree = 1000)
print(rf3)
```

## 交叉验证选择重要特征

```{r}
set.seed(8) # 随机数据保证结果可重复，必须
# rfcv是随机森林交叉验证函数：Random Forest Cross Validation
result = rfcv(t(otu_sub1), metadata_train1$Group_big, cv.fold=5)
# 查看错误率表，31时错误率最低，为最佳模型
result$error.cv
# 绘制验证结果 
with(result, plot(n.var, error.cv, log="x", type="o", lwd=2))

# 多次绘制
## 建立数据框保存多次结果
error.cv0 = data.frame(num = result$n.var, error.1 =  result$error.cv)
## 指定随机数循环10次
for (i in 1:(1+9)){
  print(i)
  set.seed(i)
  result= rfcv(t(otu_sub1), metadata_train1$Group_big, cv.fold=5) #  scale = "log", step = 0.9
  error.cv0 = cbind(error.cv0, result$error.cv)
}
error.cv0 
```

## 绘制交叉验证曲线

```{r}
# 提取x轴标签
n.var = error.cv0$num
# 提取y轴数据+标签
# error.cv = error.cv0[,2:6]
# colnames(error.cv) = paste('err',1:5,sep='.')
error.cv = error.cv0[,2:11]
colnames(error.cv) = paste('err',1:10,sep='.')


# 添加均值
err.mean = apply(error.cv,1,mean)
# 合并新的数据库，x+error+mean
allerr = data.frame(num=n.var,err.mean=err.mean,error.cv)
# number of otus selected 人为在图中观察的结果，30几乎为最低，且数量可接受
optimal = 15

# 图1：机器学习结果交叉验证图，选择Top features
# 图中 + 5条灰色拆线+1条黑色均值拆线+一条最优垂线+X轴对数变换
write.table(allerr, file = rfcv_filename_txt, sep = "\t", quote = F, row.names = T, col.names = T)

p = ggplot() + # 开始绘图
  geom_line(aes(x = allerr$num, y = allerr$err.1), colour = 'grey') + # 5次验证灰线 
  geom_line(aes(x = allerr$num, y = allerr$err.2), colour = 'grey') + 
  geom_line(aes(x = allerr$num, y = allerr$err.3), colour = 'grey') + 
  geom_line(aes(x = allerr$num, y = allerr$err.4), colour = 'grey') + 
  geom_line(aes(x = allerr$num, y = allerr$err.5), colour = 'grey') + 
  geom_line(aes(x = allerr$num, y = allerr$err.6), colour = 'grey') + # 5次验证灰线 
  geom_line(aes(x = allerr$num, y = allerr$err.7), colour = 'grey') + 
  geom_line(aes(x = allerr$num, y = allerr$err.8), colour = 'grey') + 
  geom_line(aes(x = allerr$num, y = allerr$err.9), colour = 'grey') + 
  geom_line(aes(x = allerr$num, y = allerr$err.10), colour = 'grey') + 
  geom_line(aes(x = allerr$num, y = allerr$err.mean), colour = 'black') + # 均值黑线
  geom_vline(xintercept = optimal, colour='black', lwd=0.36, linetype="dashed") + # 最优垂线
  coord_trans(x = "log2") + # X轴对数变换和刻度
  scale_x_continuous(breaks = c(1, 2, 5, 10, 20, 30, 50, 100, 200)) + # , max(allerr$num)
  labs(title=paste('Training set (n = ', dim(t(otu_sub1))[1],')', sep = ''), 
       x='Number of Classes ', y='Cross-validation error rate') + 
  annotate("text", x = optimal, y = max(allerr$err.mean), label=paste("optimal = ", optimal, sep="")) + theme_bw()
p  
ggsave(p, file = rfcv_filename_pdf, width = 180, height = 120, unit = 'mm')
```

## 特征重要性可视化

```{r}
## 预览和保存特征贡献度
imp= as.data.frame(rf3$importance)
imp = imp[order(imp$MeanDecreaseAccuracy, decreasing = T),]
# bbb = head(imp,n=optimal)
write.table(imp,file = importance_filename_txt,quote = F,sep = '\t', row.names = T, col.names = T)
# 简单可视化，比较丑
# varImpPlot(rf, main = "Feature importance",n.var = optimal, bg = par("bg"), color = par("fg"), gcolor = par("fg"), lcolor = "gray" )

# 图2. Feature重要性：绘制条形图+门属性着色

# 读取所有feature贡献度
imp2 = read.table(importance_filename_txt, header=T, row.names= 1, sep="\t") 
# 分析选择top20分组效果最好，参数显示数量
imp2 = head(imp2, n = optimal)
imp2 = imp2[order(imp2$MeanDecreaseAccuracy, decreasing = F),]
# 简化全名，去掉界
imp2$Family = gsub("Bacteria\\|","",rownames(imp2))
# 添加门用于着色(删除竖线后面全部)
imp2$Phylum = gsub("\\|.*","",imp2$Family)

# 设置顺序
imp2$Family = factor(imp2$Family, levels = imp2$Family)

# 图2. 绘制物种类型种重要性柱状图
p = ggplot(imp2, aes(x = Family, y = MeanDecreaseAccuracy, fill = Phylum)) +   
  geom_bar(stat = "identity") + 
  coord_flip() + theme_bw()
p
#ggsave(top_feautre_filename, p, width=100*2.5, height=59*2, unit='mm')
ggsave(top_feautre_filename, p, width=10, height=5)
# 名称不一定唯一，需要手动修改

#  简化全名(只保留最后，有重名不可用，可选)
# imp$Family = gsub(".*\\|","",imp$Family)
# imp$Family = factor(imp$Family, levels = imp$Family)
# p = ggplot(imp, aes(x = Family, y = MeanDecreaseAccuracy, fill = Phylum)) +   
#   geom_bar(stat = "identity") + 
#   coord_flip() + theme_bw()
# p
# ggsave(paste("top_feautre",".pdf", sep=""), p, width=89*1.5, height=59*1.5, unit='mm')
```

## 测试集独立验证

如果第一地点数据量足够大，可以取出1/2到1/3进行同一地点的独立验证。方法相同。

筛选测序集样品

```{r}
test = subset(metadata_f, Set %in% c("BHDB", "CMDB")) 

#test = metadata_f[!idx3,]
metadata_test = test
summary(metadata_test)
idx4 = rownames(metadata_test) %in% colnames(table)
metadata_test = metadata_test[idx4,]
otu_sub2 = table[,rownames(metadata_test)]

# 转置，并添加分组信息
otutab_t2 = as.data.frame(t(otu_sub2))
otutab_t2$Group_big = metadata_test[rownames(otutab_t2),]$Group_big
```


基于训练集随机森林模型验证

```{r}
set.seed(333)
otutab.pred = predict(rf3, t(otu_sub2) )  
pre_tab = table(observed=otutab_t2[,"Group_big"],
                predicted=otutab.pred) 
pre_tab

```

可视化验证结果

```{r}
# 整理样本原始分组和预测分类
predict = data.frame(group = otutab_t2[,"Group_big"], predicted=otutab.pred)

# 保存预测结果表
write.table("SampleID\t", file=prediction_filename,append = F, quote = F, eol = "", row.names = F, col.names = F)
write.table(predict, file = prediction_filename,append = T, quote = F, row.names = T, col.names = T, sep = "\t")

# 转换为数值可视化
# 预测准确标为1，错误标为0
predict$result = ifelse(predict$group == predict$predicted, 1, 0)
# IND=1, TEJ=2
predict$predict = ifelse(predict$predicted == "BHD", 1, 2)
# Set sample number in each row
column1 = 3
column = 3

AA1 = predict[predict$group=="BHD",]$predict
length(AA1)
row = round(length(AA1)/column1 + 0.5)
row
i = column1 * row - length(AA1)
AA1 = c(AA1, rep(NA, i))
matrix1 = matrix(AA1, nrow = row, ncol = column1, byrow = T)
pheatmap(matrix1, cluster_rows = F, cluster_cols = F, cellwidth = 15, cellheight = 12)
pheatmap(matrix1, cluster_rows = F, cluster_cols = F, cellwidth = 18, cellheight = 18,filename = testfilename1)
# Draw TEJ prediction result
BB1 = predict[predict$group=="CMD",]$predict
length(BB1)
row = round(length(BB1)/column + 0.5)
i = column * row - length(BB1)
BB1 = c(BB1, rep(NA, i))
matrix2 = matrix(BB1, nrow = row, ncol = column, byrow = T)
pheatmap(matrix2, cluster_rows = F, cluster_cols = F, cellwidth = 15, cellheight = 12)
# 保存图片
pheatmap(matrix2, cluster_rows = F, cluster_cols = F, cellwidth = 18, cellheight = 18, filename = testfilename2)



## 将生成的文件放入指定文件夹
tarDir <- result_file_names
dir.create(tarDir)

# 选取含特定字符创的文件（本例选取后缀为csv的所有文件）
tobeCopy1 <- list.files(".", pattern="*_rf.txt")
tobeCopy2 <- list.files(".", pattern="*_rf.pdf")
# 复制选中文件
sapply(tobeCopy1,function(x){file.copy(paste(".",x,sep="/"),tarDir,overwrite = TRUE)})
sapply(tobeCopy2,function(x){file.copy(paste(".",x,sep="/"),tarDir,overwrite = TRUE)})

# 删除转移过的文件
allfile = dir()
txtfile1 <- grep("*_rf.txt",allfile)
txtfile2 <- grep("*_rf.pdf",allfile)
file.remove(allfile[txtfile1])
file.remove(allfile[txtfile2])





```

使用此脚本，请引用下文：

If used this script, please cited:

**Yong-Xin Liu**, Lei Chen, Tengfei Ma, Xiaofang Li, Maosheng Zheng, Xin Zhou, Liang Chen, Xubo Qian, Jiao Xi, Hongye Lu, Huiluo Cao, Xiaoya Ma, Bian Bian, Pengfan Zhang, Jiqiu Wu, Ren-You Gan, Baolei Jia, Linyang Sun, Zhicheng Ju, Yunyun Gao, **Tao Wen**, **Tong Chen**. 2023. EasyAmplicon: An easy-to-use, open-source, reproducible, and community-based pipeline for amplicon data analysis in microbiome research. **iMeta** 2: e83. https://doi.org/10.1002/imt2.83

Copyright 2016-2023 Yong-Xin Liu <liuyongxin@caas.cn>, Tao Wen <taowen@njau.edu.cn>, Tong Chen <chent@nrc.ac.cn>